#import PyPDF2
import os
import openai
import sys
from langchain.llms import OpenAI
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.retrievers import ContextualCompressionRetriever
from langchain.document_loaders import Docx2txtLoader
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv()) # read local .env file
embedding = OpenAIEmbeddings()
sys.path.append('../..')
openai.api_key  = os.environ['OPENAI_API_KEY']

def load_files(files: list):
    """
    Load and process a list of files, returning a list of document loaders.

    Args:
        files (list of str): A list of file paths to be loaded.

    Returns:
        list: A list of document loaders for supported file types (e.g., PDF, DOC, DOCX).

    This function takes a list of file paths and loads them using specific document loaders based on their file extensions.
    It supports PDF, DOC, and DOCX files, using the `PyPDFLoader` for PDF files and `Docx2txtLoader` for DOC and DOCX files.
    Supported file types are determined based on the file extension.

    Example:
        files_to_load = ["document1.pdf", "document2.docx", "document3.doc"]
        document_loaders = load_files(files_to_load)
        for loader in document_loaders:
            # Process the loaded documents here

    Note:
        - Make sure to provide valid file paths in the 'files' list.
        - Document loaders for other file types can be added in a similar fashion.
    """
    
    # Initialize an empty list to store document loaders
    loaders = []

    # Iterate through the provided file paths
    for file in files:
        # Check if the file has a .pdf, .doc, or .docx extension
        if file.endswith(".pdf"):
            # Create a PyPDFLoader for PDF files
            loaders.append(PyPDFLoader(file))
        elif file.endswith((".doc", ".docx")):
            # Create a Docx2txtLoader for DOC and DOCX files
            loaders.append(Docx2txtLoader(file))

    # Return the list of document loaders
    return loaders

def split_docs(files: list):
    """
    Load and split multiple documents into smaller text chunks.

    Returns:
        list: A list of text chunks generated by splitting the loaded documents.

    This function loads multiple documents using the `load_files` function, and then splits the content of those documents
    into smaller text chunks using the `RecursiveCharacterTextSplitter` from the `langchain.text_splitter` module.

    Example:
        splits = split_docs()
        for chunk in splits:
            # Process each text chunk here
            print(f"Text Chunk: {chunk[:50]}...")

    Note:
        - Make sure to have the necessary document loaders for the supported file types.
        - Adjust the 'chunk_size' and 'chunk_overlap' parameters of the text splitter as needed.
    """

    # Initialize an empty list to store document content
    docs = []

    # Load documents using the 'load_files' function
    loaders = load_files(files)
    
    # Iterate through the loaded document loaders
    for loader in loaders:
        # Load the content of each document and extend the 'docs' list
        docs.extend(loader.load())

    # Initialize the text splitter with specific parameters
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=150
    )

    # Split the loaded documents into smaller text chunks
    splits = text_splitter.split_documents(docs)

    # Return the list of text chunks
    return splits

def load_embeddings(persist_directory: str, files: list, vectordb=None):
    """
    Create document embeddings and store them in a vector database.

    Args:
        persist_directory (str): The directory where the embeddings and vector database will be saved.

    Returns:
        Chroma: A Chroma vector database containing document embeddings.

    This function generates document embeddings from text chunks obtained by splitting documents.
    It uses the Chroma vector store from the 'langchain.vectorstores' module to store these embeddings in the specified
    'persist_directory'.
   
    Example:
        vector_db = create_embeddings('/path/to/your/persist_directory')
        # Use 'vector_db' to perform similarity searches or other vector-based operations.

    Note:
        - Make sure that the 'split_docs' function and 'embedding' are defined and available before calling this function.
        - Ensure that 'persist_directory' is writable and has enough storage space for the embeddings and vector database.
    """
    # Split documents into smaller text chunks
    splits = split_docs(files)

    embedding = OpenAIEmbeddings()
    # Create a Chroma vector database from the document splits
    if vectordb is not None and vectordb._collection.count() > 0:
        vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
        return vectordb  # Return the existing 'vectordb' if it contains items
    else:
        # Split documents into smaller text chunks
        splits = split_docs(files)

        # Create a new Chroma vector database
        vectordb = Chroma.from_documents(
            documents=splits,
            embedding=embedding,  
            persist_directory=persist_directory  # The directory to save the vector database
        )

        return vectordb

def get_moderation(question):
  """
  Check if the question is safe to ask the model

  Parameters:
    question (str): The question to check

  Returns a list of errors if the question is not safe, otherwise returns None
  """

  errors = {
        "hate": "Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.",
        "hate/threatening": "Hateful content that also includes violence or serious harm towards the targeted group.",
        "self-harm": "Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.",
        "sexual": "Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).",
        "sexual/minors": "Sexual content that includes an individual who is under 18 years old.",
        "violence": "Content that promotes or glorifies violence or celebrates the suffering or humiliation of others.",
        "violence/graphic": "Violent content that depicts death, violence, or serious physical injury in extreme graphic detail.",
  }
  response = openai.Moderation.create(input=question)
  if response.results[0].flagged:
      # get the categories that are flagged and generate a message
      result = [
        error
        for category, error in errors.items()
        if response.results[0].categories[category]
      ]
      return result
  return None


def doc_chat(vectordb, model_name: str, question: str, return_docs=True):
    """
    Retrieve an answer to a user's question using a document-based chatbot.

    This function uses a document-based chatbot to answer a user's question by
    searching for relevant information in a collection of documents.

    Args:
        vectordb (VectorDatabase): A database containing document embeddings.
        model_name (str): The name of the chatbot model to use.
        question (str): The user's question.
        return_docs (bool, optional): Whether to return source documents with the answer.
            Default is True.

    Returns:
        str: The chatbot's answer to the user's question.
    """
   
    # Initialize the chatbot with the specified model and settings
    llm = ChatOpenAI(model_name=model_name, temperature=0)

    # Define a template for formatting the conversation prompt
    template = """Use the following pieces of context to answer the question at the end.
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    Use three sentences maximum. Keep the answer as concise as possible.
    Always say "thanks for asking!" at the end of the answer. 
    {context}
    Question: {question}
    Helpful Answer:"""
    # Create a prompt template for the chatbot's input
    QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"], template=template)

    # Initialize a RetrievalQA chatbot for document retrieval
    qa = RetrievalQA.from_chain_type(
        llm,
        retriever=vectordb.as_retriever(),
        return_source_documents=return_docs,
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    )
    # Initialize a ConversationBufferMemory to store chat history
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )
    # Create a ConversationalRetrievalChain using the chatbot, retriever, and memory
    retriever = vectordb.as_retriever()
    qa = ConversationalRetrievalChain.from_llm(
        llm,
        retriever=retriever,
        memory=memory
    )
    # Retrieve an answer to the user's question
    result = qa({"question": question})

    # Return the chatbot's answer
    return result["answer"]