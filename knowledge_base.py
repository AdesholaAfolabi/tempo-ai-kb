import os
import openai
from dotenv import load_dotenv, find_dotenv
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

load_dotenv(find_dotenv())  # Read local .env file
openai.api_key = os.environ['OPENAI_API_KEY']

directory = 'knowledge_docs'

def load_files(files):
    """
    Load and process a list of files, returning document loaders.

    Args:
        directory (str): Directory where the files are located.
        files (list of str): List of file paths to be loaded.

    Returns:
        list: List of document loaders for supported file types (PDF, DOC, DOCX).
    """
    loaders = []

    for file in files:
        file_path = os.path.join(directory, file)
        if file_path.endswith(".pdf"):
            loaders.append(PyPDFLoader(file_path))
        elif file_path.endswith((".doc", ".docx")):
            loaders.append(Docx2txtLoader(file_path))

    return loaders

def split_docs(files):
    """
    Load and split multiple documents into smaller text chunks.

    Returns:
        list: List of text chunks generated by splitting the loaded documents.
    """
    docs = []
    loaders = load_files(files)

    for loader in loaders:
        docs.extend(loader.load())

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
    splits = text_splitter.split_documents(docs)

    return splits

def load_embeddings(persist_directory, files, vectordb=None):
    """
    Create document embeddings and store them in a vector database.

    Args:
        persist_directory (str): Directory to save the embeddings and vector database.
        files (list): List of file paths to process.
        vectordb (Chroma, optional): Existing vector database. If provided, new embeddings will be added to it.

    Returns:
        Chroma: Chroma vector database containing document embeddings.
    """
    splits = split_docs(files)

    embedding = OpenAIEmbeddings()

    if vectordb is not None and vectordb._collection.count() > 0:
        vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
        return vectordb
    else:
        vectordb = Chroma.from_documents(
            documents=splits,
            embedding=embedding,
            persist_directory=persist_directory
        )

        return vectordb

def get_moderation(question):
    """
    Check if the question is safe to ask the model.

    Args:
        question (str): The question to check.

    Returns:
        list: A list of errors if the question is not safe, otherwise returns None.
    """
    errors = {
        "hate": "Content that expresses, incites, or promotes hate based on various factors.",
        "hate/threatening": "Hateful content that includes violence or serious harm.",
        "self-harm": "Content that promotes or depicts acts of self-harm.",
        "sexual": "Content meant to arouse sexual excitement or promote sexual services.",
        "sexual/minors": "Sexual content that includes an individual under 18 years old.",
        "violence": "Content that promotes or glorifies violence or celebrates suffering.",
        "violence/graphic": "Violent content that depicts death, violence, or injury in extreme detail.",
    }

    response = openai.Moderation.create(input=question)
    if response.results[0].flagged:
        result = [
            error
            for category, error in errors.items()
            if response.results[0].categories[category]
        ]
        return result
    return None

def doc_chat(vectordb, model_name, question, return_docs=True):
    """
    Retrieve an answer to a user's question using a document-based chatbot.

    Args:
        vectordb (Chroma): A database containing document embeddings.
        model_name (str): Name of the chatbot model.
        question (str): The user's question.
        return_docs (bool, optional): Whether to return source documents with the answer (default is True).

    Returns:
        str: The chatbot's answer to the user's question.
    """
    llm = ChatOpenAI(model_name=model_name, temperature=0)

    template = """Use the following pieces of context to answer the question at the end.
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    {context}
    Question: {question}
    Helpful Answer:"""
    # QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"], template=template)

    # qa = RetrievalQA.from_chain_type(
    #     llm,
    #     retriever=vectordb.as_retriever(),
    #     return_source_documents=return_docs,
    #     chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    # )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )

    retriever = vectordb.as_retriever()
    qa = ConversationalRetrievalChain.from_llm(
        llm,
        retriever=retriever,
        memory=memory
    )

    result = qa({"question": question})

    return result["answer"]
