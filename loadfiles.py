from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader

def load_files(files: list):
    """
    Load and process a list of files, returning a list of document loaders.

    Args:
        files (list of str): A list of file paths to be loaded.

    Returns:
        list: A list of document loaders for the supported file types (e.g., PDF).

    This function takes a list of file paths and loads them using specific document loaders based on their file extensions.
    It is designed to work with PDF files and uses the `PyPDFLoader` from the `langchain.document_loaders` module
    to load PDF files. Supported file types are determined based on the file extension.

    Example:
        files_to_load = ["document1.pdf", "document2.txt", "document3.pdf"]
        document_loaders = load_files(files_to_load)
        for loader in document_loaders:
            # Process the loaded documents here
            document = loader.load_document()
            print(f"Loaded document: {document.title}")

    Note:
        - Make sure to provide valid file paths in the 'files' list.
        - Document loaders for other file types can be added in a similar fashion.
    """

    # Initialize an empty list to store document loaders
    loaders = []

    # Iterate through the provided file paths
    for file in files:
        # Check if the file has a .pdf extension
        if file.endswith(".pdf"):
            # Create a PyPDFLoader for PDF files
            loaders.append(PyPDFLoader(file))

    # Return the list of document loaders
    return loaders

def split_docs():
    """
    Load and split multiple documents into smaller text chunks.

    Returns:
        list: A list of text chunks generated by splitting the loaded documents.

    This function loads multiple documents using the `load_files` function, and then splits the content of those documents
    into smaller text chunks using the `RecursiveCharacterTextSplitter` from the `langchain.text_splitter` module.

    Example:
        splits = split_docs()
        for chunk in splits:
            # Process each text chunk here
            print(f"Text Chunk: {chunk[:50]}...")

    Note:
        - Make sure to have the necessary document loaders for the supported file types.
        - Adjust the 'chunk_size' and 'chunk_overlap' parameters of the text splitter as needed.
    """

    # Initialize an empty list to store document content
    docs = []

    # Load documents using the 'load_files' function
    loaders = load_files([])
    
    # Iterate through the loaded document loaders
    for loader in loaders:
        # Load the content of each document and extend the 'docs' list
        docs.extend(loader.load())

    # Initialize the text splitter with specific parameters
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=150
    )

    # Split the loaded documents into smaller text chunks
    splits = text_splitter.split_documents(docs)

    # Return the list of text chunks
    return splits

